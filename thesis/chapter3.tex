\chapter{Model Design and Methods Used}

\section{Getting and Pre-processing the Data}

\subsection{Choosing the dataset}

First, when this project was in development, we only had access to a 
single students dataset, i.e. students performance grades from the 
Analysis of Algorithms course, which took place in the Fall 2015 semester. 
As progress was being made with the implementation, it became clear that the 
dataset could not be practically used, due to its high amount of noisy data. 
The noise was present in the data because of the small size of the 
dataset (137 students were taken into consideration) and also, 
the non-trivial distribution of final grades over the semester activity of students. 
The grading subjectivity of teaching assistants combined with a new 
experimental grading method for the course were also important factors in 
this negative result. Hence, with this much noise in the data, the models 
have not been able to correctly label the examples and, with more complicated 
models, there was a high tendency for {\it overfitting} the data. 

Later, as the second semester was ending, we had 
access to another dataset of students and decided to switch to it. We also 
kept the students final grades for the first course to use them as attributes 
for the new dataset, in order to increase accuracy. In the $5^{th}$ 
chapter of this thesis, a detailed analysis will be made between these two 
datasets to show how they differ and what is wrong with the first one.

An important note must be made here. To increse the number of examples for 
our dataset, we could have combined those two above or could have aggregated 
students from different years into a single dataset but, there are two main 
reasons we did not do this: one, because the grading method and the number 
of semester assessments were different from one year to another and, second, 
because the two courses are conceptually different: one is theoretical, the other 
is more practical. Also, combining those two sets of examples would just have brought 
more noise to the second dataset. 

Given this situation, the data that is currently used in this thesis comes from our Computer Science 
Undergraduate course, i.e. Programming Paradigms from $2^{nd}$ year during 
the Spring 2016 semester. For that semester, we had a total record of almost 200  
registered students, but we removed those whose attributes were missing or were not relevant 
(the ones that had a very low performance).
Since we are interested in classifying students based on their exam grades and final grades, 
also implying the classification in {\it passed} and {\it failed} classes, 
we kept some students that had a poor grade during the semester and 
could not participate at the final exam, although they were close to that point. 
Having said this, we are left with a total of 143 relevant student records. 
These records must further be split in two sets: the training set - the set 
that is used for building the model and the testing set - the set used for 
testing the accuracy and measure the performance indicators of the model. 

There is not a standard recipe of how to divide these two sets, but it is 
recommended that the training set should have a proportion between 60\% and 
80\% of the total dataset's number of examples. For empirical reasons, the 
ratio of 80-to-20 percent was chosen for our dataset and the examples were sorted 
alphabetically based on the student's surname (the order is independent from the 
features and labels).
The current dataset is not uniformly distributed over the {\it failed/passed} 
classes. There are 40\% students that passed the course and 60\% that failed it.
This proportion is helpful in studying the prediction models (they must 
predict the failed students with a slightly higher accuracy). In more detail, 
from those 143 analyzed students, 57 of them had a positive grade (>=5) and 86 a 
negative one (<=4). From the 86 students that did not pass the course, 
55 of them failed the final exam and 31 of them failed the course during 
the semester (and did not take part in the final examination process). 
The reason we kept those 31 students in the datased was to give the ML 
model more data examples, thus increasing the accuracy when predicting 
{\it failed} students. 

\subsection{Dataset Structure}
\label{data_str}

To build the dataset, several sources of information were used. The first 
trivial one is the course semester activity of the students. The second, 
comes from performance of students achieved at past courses relevant to ours 
(chosen in different semesters in order to make them as independent from each 
other as possible), from a topic perspective. The courses that we got the 
grades from are:
\begin{itemize}
\item Computer Programming (PC): first year, first semester
\item Data Structures (SD): first year, second semester
\item Analysis of Algorithms (AA): second year, first semester
\end{itemize}
Lastly, the third subset of entire dataset was taken from our on-line 
e-learning course platform\footnote{Moodle, the Open-source learning platform}. 
There, we had access to all the students activity logs recorded during the 
course progress. The logs were downloaded as {\it .csv} file format and then, 
aggregated in four categories for meaningful results.

In Table \ref{table_1} there is a listing with all the attributes (features) 
belonging to the first subset of features - the course semester activity.
There are a total of 10 initial features (will address later on this problem 
with the number of features). The weight characteristic of the features 
represents the actual maximum points that a student can get from that 
assignment (they are not equal, but are summing for a total of 6.0 semester 
points out of 10), and was not used for building the ML models.

Table \ref{table_logs} describes the third subset of our dataset's features: 
the Moodle logs. Activities including homework forum posts and views ($x_{1}$), active days 
($x_{2}$), course resources opened ($x_{3}$) and total number of logs per 
user ($x_{4}$) were used along with the rest of features (together and 
separately) in the learning models.

\begin{table}[ht]
\centering
\begin{tabular}{l*{10}{c}r}
\toprule
Feature            & Description & Type & Range & Weight \\
\hline
t\_1               & Test 1 grade & Float & 0-10 & 0.25 \\
t\_2               & Test 2 grade & Float & 0-10 & 0.25 \\
t\_3               & Test 3 grade & Float & 0-10 & 0.25 \\
t\_4               & Test 4 grade & Float & 0-10 & 0.25 \\
t\_f               & Final test grade & Float & 0-10 & 1.0 \\
hw\_1              & Homework 1 grade & Float & 0-1 & 1.0 \\
hw\_2              & Homework 2 grade & Float & 0-1 & 1.0 \\
hw\_3              & Homework 3 grade & Float & 0-1 & 1.0 \\
lab                & Lab activity & Float & 0-0.5 & 0.5 \\
lecture            & Lecture activity & Float & 0-0.5 & 0.5 \\
\bottomrule
\end{tabular}
\caption[Dataset Details]{Dataset Semester Attribute Details}
\label{table_1}
\end{table}

Next, we need to provide the values used for labelling the examples. Since this 
thesis treats both classification and regression problems, different types of 
labels must be present in the dataset. For classification, we use the final 
grade (real number between 0 and 10) and the exam grade as integer values. For 
regression, only the exam/final grade is used and it will be represented as 
a float number. 

Table \ref{table_2} gives a structured view of how the 
students dataset labels are used in the studied models. For classifying the students 
into 3 classes based on their exam points or final 
grade, the real continuous interval $(0, 10]$ was split into 3 different-length subintervals: 
$(0, 5),\ [5, 7]$ and $(7, 10]$. 
For each subinterval there was assigned a class label: 0, 1 and 2, respectively. 

\begin{table}[ht]
\centering
\begin{tabular}{l*{4}{c}r}
\toprule
Feature            & Description & Type \\
\hline
x\_1               & f(forum\_posts, forum\_views) & Float \\
x\_2               & Number of active days & Float \\
x\_3               & Number of course resource views & Float \\
x\_4               & Total number of logs per user & Float \\
\bottomrule
\end{tabular}
\caption[Logs Dataset Details]{Dataset Logs Attribute Details}
\label{table_logs}
\end{table}

The $f$ function from the \ref{table_logs} table is actually a linear combination 
of the number of forum posts and forum views, as follows:

\begin{equation}
\centering
f(\#posted, \#viewed) = \#posted + \frac{\#viewed}{\alpha}
\end{equation}
where $\alpha$ is the factor representing the {\it viewed} to {\it posted} ratio of all 
the homework forum logs (40 in our case). This was chosen because we are more 
interested in forum posts (as they are more relevant), so the forum views 
are adjusted with this factor to have a smaller value.


\begin{table}[ht]
\centering
\begin{tabular}{cccc}
\toprule
\multicolumn{1}{c}{Problem type} & \multicolumn{3}{c}{Label Description} \\
\cmidrule(r){2-4}
                        & Label type  & Label Value & Label used \\
\midrule
Binary Classification   & Integer     & 0|1          & Exam/Final grade \\
3-class Classification  & Integer     & 0|1|2        & Exam/Final grade \\
Regression              & Float       & 0.0-10.0     & Exam/Final grade \\
\bottomrule
\end{tabular}
\caption[Label Structure]{Label Structure}
\label{table_2}
\end{table}

\subsection{Data Standardization}

In Machine Learning, standardization of data is a common requirement for a 
lot of models (e.g.: neural networks and SVMs). This means that, before 
applying our learning models, we must first scale the features from our dataset. 
This scaling implies that the data should have {\bf mean} 0 and {\bf standard 
deviation} 1. This is done by subtracting the mean value of each feature, then 
scale the data by dividing the features by their standard deviation (or variance, 
because standard deviation is the square root of variance, so they are equal).

\begin{equation}
\centering
X_{scaled} = \frac{X-\overline{X}}{\sigma}
\end{equation}
where $X$ is a vector of values from one feature, $\overline{X}$ is the mean of $X$ and 
$\sigma$ represents the standard deviation of $X$.

In Figure \ref{fig.figure2}\footnote{http://cs231n.github.io/neural-networks-2} we can 
see a straight-forward visualization of how the data is represented before and after 
the preprocessing techniques:

\insfigshw{figure2.png}%
    {Data preprocessing pipeline}%
    {Data preprocessing pipeline}%
    {fig.figure2}{0.8}

Data standardization is important if we are comparing features that have different 
ranges, but it is also necessary for some of the machine learning models. That is 
because these models use the {\it gradient descent\footnote{an optimization algorithm
that is used to find a local minimum of a cost function}} algorithm, which is 
sensitive to feature scaling.

It is worth noting that the mean and standard deviation values must come only 
from the training data and the transformation must be done on both training and 
testing sets, for meaningful results. The reason is that, in general, the built 
Machine Learning model is applied on unseen data (on real-time data), which is 
not available when the model is built. So, for accurate calculations on the 
model's performance and generalization, we must restrict the computation of 
mean and variance only on the training examples.

\section{Generating new features}

\subsection{The shape of a general dataset}
\label{dataset_gen}

Because one of the purposes of this work is to apply the learning model 
on future datasets (from the same course or even from other courses), 
we must first generalize the model, i.e. find a general set of features. To do this, 
the dimension of the feature space must have a constant value along multiple datasets 
and each feature must have the same {\it meaning}. Therefore, we are going 
to divide our current feature space discussed in subsection \ref{data_str} 
into several categories, based on their similarity. Besides the above stated 
purpose, an advantage of this feature modelling is to decrease the dimension 
of the feature space. Having a small, but meaningful dimension of the input, 
the learning models can sometimes perform better, mainly when the number of 
examples in the dataset is small. The reasons for this statement are described 
in the following subsection. 

Since almost every course has a semester grading method based on homeworks, 
tests, lecture and lab activity, the split visible in table \ref{table_agg} 
comes natural. With this, the dataset now has a constant number of features: 
five. If the Moodle logs features (which also have a constant dimension 
regardless of the dataset used) are further added to this new feature space, 
there will be a total of nine features. This way, we can use every student 
dataset, transform it to have this structure and then, apply the model.

A disadvantage of this generalization is that we lose information about 
features that get aggregated together in a new one. Sometimes this is useful, 
for example when we want to see what homework or what test was the most relevant 
in the student's exam or final grade.

\begin{table}[ht]
\centering
\begin{tabular}{l*{6}{c}r}
\toprule
Feature              & Description & Type & Range \\
\hline
hw\_agg               & Homework points    & Float & 0-1 \\
t\_agg                & Test points        & Float & 0-10 \\
past\_agg             & Past results score & Float & 0-10 \\
lab                   & Lab activity       & Float & 0-0.5 \\
lecture               & Lecture activity   & Float & 0-0.5 \\
\bottomrule
\end{tabular}
\caption[Dataset Agg]{Dataset Semester Aggregated Features}
\label{table_agg}
\end{table}

Note: the homework, test, and past results aggregated features are calculated as 
a weighted arithmetic mean between their components.

\subsection{Curse of Dimensionality}

In general, the number of examples and the dimensionality of each example 
(the number of features per example) are correlated, taking into consideration 
the accuracy of the trained model. The {\it Hughes phenomenon}\cite{hughes} 
tells us that if we have a constant number of training examples, the ability 
of the model's prediction decreses when the dimensionality increses over an 
optimal value. This is also called the {\bf Curse of Dimensionality} and it 
can lead to {\it overfitting} the dataset - the model has a low power of 
generalization. Finding the best number of features can be a very hard problem 
(as it requires a lot of manual testing). Actually, this is an {\it intractable} 
problem, because we need to generate all possible combinations of features and 
find the optimal one. This could easily be avoided now by using Feature Selection 
tools. For example, {\bf Random Forests} are very good models at selecting 
features that provide the best accuracy to the model. This will be analyzed 
with more details in the next sections of the thesis. 

So, supposing that we have $M$ number of examples in the training set and 
the feature tensor is one-dimensional, if we add another dimension to the 
tensor (another feature), ideally, we need to square the training examples. 
By induction, the number of training example grows exponentially with the 
dimension of the feature tensor. The reason behind this, is that when adding 
more features to the dataset by keeping the same number of examples, the 
space where our examples are distributed becomes sparser. So, in order to 
keep the same sparseness of the space, we must fill the higher dimensional 
space with more data, and that data must grow exponentially as the dimension 
of the space increases. Keeping the same size of the dataset will result in 
overfitting the data, which is bad for a model.

Figure \ref{fig.figure3}\footnote{http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/} shows a representation of how the number of feature dimensions 
affects the quality of the learning model. It can be seen that keeping the 
training examples constant and only increasing the number of features, the 
accuracy drops by an exponential rate. Finding the optimal dimension of the 
feature space requires a lot of work and testing - there is not an universal 
recipe that does this yet, but methods such as Feature Selection or Feature 
Extraction are a good place to start. 

\insfigshw{figure3.png}%
    {The Curse of Dimensionality}%
    {The Curse of Dimensionality}%
    {fig.figure3}{0.8}

\subsection{Adding Complexity to the Model}
\label{polyn}

There are situations in which is better to add complexity to our data. 
For example, when our dataset in not linearly separable using one, two, or more 
features, we can add extra dimensions to the feature tensor in order to make 
that data separable. {\bf SVMs} make use of this approach, by using multiple 
{\it kernel functions} that have the role to transform an input space in order 
to easily process data. Intuitively, a kernel is a ``shortcut'' that allows us 
to do certain computations, but without being directly involved in higher-dimensional 
calculations. Kernels can be linear functions, polynomial functions or even sigmoid 
functions. Using SVMs, we implicitly add complexity to our model.

Having tested some simple models on the above dataset, such as Linear Regression or the 
Perceptron, adding some complexity to the models was not at all a bad idea. 
A good method was the one of using {\bf Polynomial Features}, that combines the 
initial dataset's features into new nonlinear features. Polynomial Features add more 
dimensions to the feature space, but the key here is that they are correlated, 
so this can help in achieving better prediction accuracy. Here, there are two different 
options to consider:

\begin{itemize}
  \item The first one is generating a list of features (a polynomial of a certain 
  degree) from the current features. 
  Example: If we have the input given as $(X_1, X_2)$, after the polynomial 
  tranformation the example becomes $(1, X_1, X_2, X_1*X_2, X_1^2, X_2^2)$
  \item The second approach was to consider only interactions between the 
  features for building a polynomial with the same degree as the number of 
  initial features.
  Example: The features $(X_1, X_2, X_3)$ are transformed by the polynomial 
  into: $(1, X_1, X_2, X_3, X_1*X_2, X_1*X_3, X_2*X_3, X_1*X_2*X_3)$, resulting 
  in a total of $2^{N}$ final features, where $N$ is the original dimension of 
  the input space.
\end{itemize}

So, for Linear Regression, the input features are transformed using the first 
method (to generate non-linear functions like polynomials of degree 2 or 3).
This ``trick'' allows us to use simple linear models that are trained on actual 
non-linear combinations of the data and are faster than other complex 
non-linear models.
Supposing that we want to train our student's dataset using Linear Regression 
with Polynomial Features: 

Let $\tilde{y}$ be the output vector of the linear model, $x$ the input tensor,
$\omega \in \mathbb{R}^{M \cdot N}$ the coefficient tensor (a two-dimensional vector) 
and $\beta$ the vector bias. The model computes the following equation, making use of 
the ``least squares'' method for calculating 
$\omega$ and $\beta$:
\begin{equation}
\label{lin_reg}
\centering
\tilde{y}(\omega, x) = \omega \cdot x + \beta
\end{equation}
where $x = (x_1,x_2,\dots,x_{9}).$
When we add the polynomial features, $x$ is transformed like this:
\begin{center}
$x_{nonlinear} = (x_1,x_2,\dots,x_{9},\dots,x_i \cdot x_j,\dots,x_1^2,x_2^2,\dots,x_{9}^2);$ 
 $i, j = \overline{1,9}, i < j$
\end{center}
The size of the new feature space is: $9+9+C_{9}^2=54$ \\
Substituting $x_{nonlinear}$ in Equation \ref{lin_reg}, we obtain:
\begin{equation}
\centering
\tilde{y}(\omega', x_{nonlinear}) = \omega' \cdot x_{nonlinear} + \beta'
\end{equation}
We can observe from the above equation that the linearity is still preserved and the 
model can fit more complicated data now.

On the other hand, the Perceptron model was tested using both methods, although the 
second method turned to be more appropiate, i.e. the {\it interaction features} method. 
This method was also used for the MLP\footnote{Multi-Layer Perceptron} neural network 
model. With the size of the feature space of 9, adding interaction features 
provided a total of $2^{9}=512$ features.

Besides {\bf Polynomial Features}, another way to add more features from 
existing features is to use {\bf Trigonometric Features}, like $sin(x)$, 
$cos(x)$, etc. This can be useful when you only have two or three features in the 
dataset and want to increase the input size a little more to see if the model 
(e.g. neural networks) can fit the data more precisely.

It is worth saying here that this approach of making the model more complicated 
is a good thing when having a big dataset of training examples. In our case, the 
training examples have a dimension of 80\% of a total of 143 examples, which is 
approximately 115 examples. With this number, and with an input size of 9 features, 
generating polynomial features and increasing the input space to a much higher 
value does not scale very well, even if the features are correlated to each other: 
it will only make the data more complex and the model will give bad results, 
no matter how good it is, theoretically. 

\section{Visualizing the dataset}

Machine Learning can be very counter-intuitive when we are dealing with 
the number of dimensions. Often, it implies working with hundreds, or even 
thousands of dimensions, and our human mind cannot reason easy about this 
(or not at all) when it comes to visualizing/imagining what is happening 
with the data or how it looks. Humans can think with no problem in two 
or three dimensions (even four, with some effort), so researchers in this 
field came up with some useful tools that do a {\it dimensionality reduction} 
of the data. This means transforming data from higher-dimension space to 
a human-meaningful lower-dimension space, with the purpose of having a view 
of how the dataset looks before trying to apply some Machine Learning models 
ot it. Also, another reasing for the reduction of the input space is to decrease 
the number of features in an optimal way (find the most important ones or create 
new fewer ones from the existing features), so the models can provide better 
results.

There are many tools that are used for the dimensionality reduction. Some 
examples are: {\bf PCA\footnote{Principal Component Analysis}, MDS\footnote
{Multidimensional Scaling} and t-SNE\footnote{t-Distributed Stochastic Neighbor 
Embedding}}. If visualization is what is needed, {\bf Graph Based Visualizations} 
models are very helpful, as they give important insights into the structure 
of the data, i.e. how the points are connected to each other.

\subsection{PCA}

For this work, the {\it PCA} technique was used to get information about our 
dataset and see how it looks, visually. PCA was not used for {\it feature 
extraction} purposes, since the aim was to focus on the models and current 
features and to get useful insights about them and their role in the student's 
performance. 

What PCA does is to get the original input space and apply to it an orthogonal 
linear transformation in such a way that it will reduce the dimension of the 
input space and the new points will be spreaded the most in the new space. 
The reason behind this is to capture the most possible {\it variance} of the 
new data, so we can get a better look at them. Variance measures how much 
the data is distributed across the space, i.e. how far a set of points are 
spread out from their central point (thei mean value).

For example, considering the dataset fits in an {\it N-dimensional} hypercube, 
PCA will find the optimal angle to look at the data and then will project that data 
to two or three orthogonal axes, so it can be visualized. That angle will 
give the most variation in the data. However, the most variation does not 
always imply that the new dimensions can be used as new, alternative features 
for the learning algorithms, mostly when the dataset is not classifiable (it 
has a very high amount of noise).

\subsection{Looking at the data}

Since our dataset's feature space has a minimum dimension of 4 (when 
using just the Moodle logs features) and a maximum dimension of 17 (when 
using all the possible features - past results, semester grades and 
Moodle logs), PCA was applied multiple times for different subsets of the 
feature space with the goal of reducing its dimension. In this subsection, 
the dataset described in subsection \ref{dataset_gen} was used as an example. 

\insfigshw{figure4.png}%
    {2D PCA of the model}%
    {2D PCA of the model}%
    {fig.figure4}{0.8}

In figure \ref{fig.figure4}, it can be seen how the points are spread across 
the two principal axes in order to have the most variance. The first one, i.e. 
the horizontal one, represents the first principal component (with the highest 
variance) and the second one (the vertical axis) count for the second principal 
component (which has the second-highest variance). After the plot was made, 
the points were colored based on their class membership. The red dots represent 
the students that failed the course and the green dots the ones that passed 
(for this example only the binary split of the examples was taken into consideration). 
We can see from the picture that the PCA algorithm manages to split the dataset 
without {\it knowing} about the label values. This is why PCA can be seen as 
an unsupervised learning model for classification that finds patterns in the data 
by its own. 

\insfigshw{figure5.png}%
    {3D PCA of the model}%
    {3D PCA of the model}%
    {fig.figure5}{0.8}

Figure \ref{fig.figure5} show the same PCA analysis of the same dataset, but 
this time plotted in three dimensions, only to get another viewable perspective.
The higher the number of dimensions, the more information the model has. So, 
using three instead of two dimensions for the PCA, we can get more insight into 
the structure of the data (e.g.: in three dimensions we can see if the data has 
a curvature, and this can be helpful).

Looking at the two visualizations made by the PCA analysis, some conclusions can 
be drawn about the dataset. First, we can observe that there is only a small 
amount of noise in the data, which is a good thing, because the models used will 
be able to classify students with a high accuracy. Second, PCA provides a 
safety check before going further with building the model (if we do not get a 
preliminary overview of our dataset, we will not know how well is expected from 
it to perform).

\section{The Models}
\label{models}

After building the dataset, which implied choosing the examples and the subsets 
of features, the next step is to choose a Machine Learning model (or more) to 
start training the data. The first part of this section discusses the design of 
two simple models (the {\it Perceptron} and the {\it Linear Regression} model), 
while the second subsection expands the architecture and components of a more 
complex model: {\it Artificial Neural Networks}. Finally, in the third part 
the {\it Random Forest} model is analyzed, with focus on its structure and goals. 

\subsection{Simple Models}

\subsubsection{The Perceptron}

The perceptron is a model used for binary classification. The algorithm is linear: 
it combines the input vector of features with a vector of weights that are 
adapted in the learning process, based on the sign value (-1 or +1) of the product 
between the real class label and the sign value of the linear term $\omega \cdot x + b$. 
When this process ends, the model can decide if an input belongs to a class or 
the other. Figure \ref{fig.perceptron} shows the model's simple architecture. 

\insfigshw{perceptron.png}%
    {The Perceptron}%
    {The Perceptron}%
    {fig.perceptron}{0.8}

This design was inspired by how a biological neuron works (it activates on 
certain learned thresholds) and it was the first step into developing 
{\it artificial neural networks}, which are a collection of perceptrons 
arranged in a layered network. 

From the figure, we can see certain elements of the model: $x$ and $omega$ 
represents the input vector and the vector of weights, respectively; the $bias$ 
has the role of adjusting the boundary position between the classes; $f$ and 
the {\it activation} function represents the {\it perceptron algorithm} 
and $y$ is its output, i.e. $-1$ or $1$. 

\subsubsection{Linear Regression}

The Linear Regression model uses the {\it ordinary least squares} method to 
solve an optimization problem that has the form:

\begin{equation}
\centering
min_{\omega} \| \omega \cdot x - y \|_{2}^{2}
\label{opt_pb}
\end{equation}

Starting from the standard equation $y = \omega \cdot x + \beta$, the method  
is used to find the $\omega$ and $\beta$ parameters of the equation that satisfy 
the above optimization problem. The solutions are:

\begin{equation}
\centering
\omega = (x^{T} x)^{-1} x^{T} y ;\ 
\beta = y - \omega \cdot x
\end{equation}

Note: since this an optimization problem, {\it gradient descent} cand also be 
used to find its solution, but with an iterative approach, not analytically like 
the first method. The cost function used by this algorithm has a form 
similar to the optimization problem mentioned above in equation \ref{opt_pb}: 

\begin{equation}
\centering
Loss(\omega) = \frac{1}{2}\| \omega \cdot x - y \|_{2}^{2}
\label{sq_loss}
\end{equation}
where $\frac{1}{2}$ is a factor used only as a convenient when calculating 
the first derivative of the cost function:

\begin{equation}
\centering
\frac{\partial Loss(\omega)}{\partial \omega_{j}} = \frac{\partial}{\partial \omega_{j}}\frac{1}{2}
(\omega x - y)^{2} = (\omega x - y) x_{j}, for\ all j = \overline{1, N}
\end{equation}
where $N$ is the input size of an example.

The algorithm uses this partial derivative to repeatedly update the weight vector 
$\omega$, until convergence:

\begin{equation}
\centering
\omega_{j} := \omega_{j} - \alpha \frac{\partial}{\partial \omega_{j}}Loss(\omega),
\ for\ every\ j.
\end{equation}
where $\alpha$ is the learning rate of the algorithm.

Combining this with the {\it polynomial features} discussed in subsection 
\ref{polyn}, we can extend the model in order to perform better at fitting the 
data, while still preserving its linearity.

\subsection{Neural Networks}

\subsubsection{Model Architecture}

To define a {\bf neural network}, first we have to start from the concept of 
the {\bf perceptron}, discussed in the first part of this section. To remind, 
a perceptron takes inputs of a certain size and computes a single output, 
which has a binary value (0/1 or -1/1), thus being able to classify data 
in a linear approach. 

When the dataset is too complicated to be linearly classified, the perceptron 
model is not capable of doing the job. Therefore, a more complex model had 
to be developed based on the latter, and that is {\bf neural networks}. 
A neural network is a generalization of the perceptron model, that is built up 
by not one, but many single neurons (or perceptron) arranged in a network that 
has a particular structure. The network consist of layers of neurons (groups 
of interconnected neurons). The neurons from a layer are independent to each 
other and only connected to the neurons from the next and last layers. 

\insfigshw{figure6.png}%
    {Neural network architecture}%
    {Neural network architecture}%
    {fig.figure6}{0.9}

Figure \ref{fig.figure6} shows a generic architecture of a neural network\footnote
{Sometimes called a Multi-layer Perceptron}. In the first layer, the input is given 
to the neural network. This network has a single layer of hidden units, but 
there are networks with more than one hidden layers, used at very complex 
computations, such as pattern recognition, image classification, and so on. 
The hidden layer in the figure makes four basic decisions based on the weights 
assigned to each edge between the neurons from the input layer and the ones 
from the hidden layer. The hidden layer and the output layer are also connected 
to each other, and another set of weights exists between them. The weights from 
each of the connections can also be seen as a weight matrix of size {\it number 
of hidden units} $\times$ {\it number of last layer size}. 

With each of the hidden layers, the model learns new {\it representations} of 
data, by making transformations of the space topology in which the data resides. 
The reason for this is to make the dataset easier to classify in the last layer 
of the network (in the last representation of the dataset, the model may simply construct 
a line through it). This is why the neural network models are non-linear and 
capable of predicting very complex inputs.

Since this model is capable of learning non-linear functions from the data, 
after each layer (except the input layer) there must exist an activation function 
similar to the one used in the perceptron model, otherwise the model will still 
be linear, no matter the number of hidden layers. There exists multiple activation 
functions for neural networks, but the most popular ones are the {\it sigmoid function}, 
the {\it hyperbolic tangent function} and the {\it rectifier function} (ReLU). They 
give different results based on the dataset and are chosen in the process of 
{\it cross-validation\footnote{The technique of running multiple models or 
variations of the same model to a given dataset, in order to choose the most performant one.}}. 

For the learning part of this model, {\it gradient descent} is used. If the 
model is trained on large datasets (thousands or millions of examples), a 
different version of this algorithm is implemented and that is, {\it stochastic 
gradient descent}. The classic algorithm must load the entire dataset into memory 
at each iteration, and this is not scalable if the dataset is big. On the other 
hand, stochastic gradient descent applies the same computations of a random 
subset ({\it mini-batch}) of the data at each step. This gives it a probabilistic 
flavor (hence the word {\it stochastic}) and it means that it can either find the minimum 
faster, or it may never converge to the minimum, theoretically. In practice, the error 
is neglected, as the computed value is very close to the real minimum.

\subsubsection{Usage}

In this, part we are going to discuss how the {\it neural networks} had been  
applied as part of the development of our model. 

We used neural networks for both classifying our dataset in two and three classes 
and predicting a continuous value based on the labels of the dataset pointed in table 
\ref{table_2}. For the regression type of problem (real value prediction) a single 
neuron was used in the output layer of the network, with no activation function 
(or, equivalently, with the {\it identity function} as the activation function). 

The classifier, however, used the {\it logistic function} for the binary classification 
to obtain values between 0 and 1. For this case, there is still only one neuron 
needed at the output layer. In general, for $n$-class classification, the output layer 
has $n$ neurons, and each neuron provides as value the probability for the input 
example to belong to the class represented by that neuron. But, since the 
probabilities must sum up to 1, for binary classification we only need one 
neuron in the output layer, because the second probability value can be infered 
from the first one.

The 3-class classification uses the {\it softmax} function at the output layer:

\begin{equation}
\centering
softmax(y')_{i} = \frac{e^{y'_{i}}}{\sum_{k=1}^K e^{y'_{k}}}
\end{equation}
where $y'$ is the output of the network before passing to the {\it softmax} 
layer (raw output) and K is the total number of classes. $i$ represents 
the i-th component of the class vector.

The {\it softmax} function also has the property of transforming an input of 
raw values into a probability distribution over the set of classes. So, when 
predicting a class label, the value with the highest probability is picked 
from all the class probability values.

For regression, our network uses the {\it Loss} function stated in equation 
\ref{sq_loss} as the cost function, which it will try to minimize. On the other 
hand, the both classifiers use a different cost function, since they deal with 
probabilities this time. The cost function used is called the {\it Cross-Entropy} 
function and is defined as follows:

\begin{equation}
\centering
Loss(\omega) = \frac{1}{M}\sum_{m=1}^M\sum_{k=1}^K [y_{k}^{(m)} log_2(\frac{1}{y'_{k}}) + 
(1-y_{k}^{(m)}) log_2(\frac{1}{1-y'_{k}})]
\end{equation}
here, $y'$ is the predicted probability value, $y$ the desired output (encoded as a 
{\it 1-of-k} vector\footnote{A vector which has 1 for the correct class component and 
zero in the rest of its components}), $K$ the number of classes, $M$ the number of training 
examples and $\omega$ being represented as the tensor of parameters (weights and biases) 
of the network that will get updated during the training process in order to minimize 
this loss functions.

The architecture of our model for the 3-class classification problem is 
evidentiated in figure \ref{fig.figure7}, as an example. The input layer has a 
size of 9 neurons (corresponding to our number of features), the output layer 
consists of 3 neurons (given that we are classifying the dataset in three 
categories) and the hidden layer has 10 neurons. We chose a single hidden layer 
because of the few number of training examples (if the model is too complex 
for the dataset, it would overfit the data and not generalize well).

The number of hidden layer units was decided based on some empirical rules\footnote
{www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html}:

\begin{itemize}
\item The number of hidded neurons should be between the number of input size 
and the number of output size (typically the mean of those)
\item The hidded layer size should not exceed twice the number of the input layer size
\item The size of the training examples should an upper-bound for the hidden 
\item Choosing the size of the hidden layer as $sqrt(N_{i} \cdot N_{o})$, where $N_{i}$ 
is the input size and $N_{o}$ is the output size can lead to good results\cite{n_hidden}
\end{itemize}

The hidden layer architecture is different for the other classification problem and the 
regression one. However, the same number of layers was preserved (but the size of the 
layer was changed). Also, the output size has different values: two for the binary 
classification and one for the regression. Other parts of the model (input, algorithm, 
learning rate and other model parameters) remained unchanged.

\insfigshw{figure7.png}%
    {3-class NN classifier with one hidden layer}%
    {3-class NN classifier with one hidden layer}%
    {fig.figure7}{0.9}
Note: The learning algorithm used was SGD\footnote{Stochastic Gradient Descent} 
with a learning rate value of $1E-3$.

\subsection{Random Forest}

{\it Random Forests} are probabilistic learning models based on the {\it 
Decision Trees} model. Decision Trees are simple tree structures that are 
used mainly for classifying data. The tree finds the best possible split of 
the dataset by assigning to each node a feature from the feature space. 
The feature selection is made by making an objective measurement of the 
``purity''\footnote{The purity is measured by entropy or by the {\it gini 
impurity function}} of the dataset (we select the feature that gives the best 
split of our data). Therefore, when the algorithm arrives at the leaves of 
the tree our dataset is classified, since each leaf contains the value of one 
of our classes. If we run the algorithm multiple times on the same dataset 
the results will be the same, because all the decisions taken by it are 
deterministic. Given this, the model is sensitive to small perturbations of 
the dataset, so it has the habit of overfitting. 

To conquer the limitations of the above model, there was a need for a better 
one, so Random Forests were invented. The Random Forest model extends the 
notion of Decision Trees, by using an different approach. Instead of building 
a single complicated deterministic tree, Random Forest uses a collection 
of simple trees (a forest of trees) generated from random subsets of the data. 
An overview on the structure of this model can be seen in figure \ref{fig.figure8}
\footnote{https://www.researchgate.net/figure/278677730\_fig2\_Conceptual-diagram-of-the-RANDOM 
-FOREST-algorithm-On-the-left-trees-are-trained}.

\insfigshw{figure8.png}%
    {Random Forest model}%
    {Random Forest model}%
    {fig.figure8}{0.9}

Each tree is trained with a random subset of the whole dataset. At each node, 
a small subset of all the features is chosen and the split will be made like 
it is made in the Decision Tree model (a feature will be selected from that 
random subset that will best split the data). The size of the feature 
subset is usually chosen as the square root of the total feature size. At the end of 
each random tree a table with the frequencies of all the labels will be kept 
in each leaf, based on the partitioning of the training samples. 
When testing new data with this model, the input goes through all the trees 
in the forest and each tree will provide a frequency list of the labels. 
By a majority voting method (or by averaging\footnote{Depending on the 
implementation, for example, {\it scikit-learn} uses the averaging method}), 
the model will choose the class that has the highest probability. 

Having this structure, Random Forests are good for reducing the {\it variance} 
\footnote{A measure of how the model behaves on small fluctuations of the data}
of the model, by averaging results from multiple random decision trees. 

Besides making predictions, Random Forest models also provide information about 
the importance of each feature in the prediction process. This is because each 
node in each tree works with a small random subset of all the features and 
selects the one that gives the best split. So, features that are chosen at the 
top of a tree are considered more important because they contribute to a bigger 
set of training examples than the ones from the bottom. At the end, each of the trees will 
provide a list of the most important features (the ones that were selected 
for the splits) and then, a {\it feature ranking} can be computed by aggregating 
all these local ranks. 

\subsubsection{Usage}

We used the Random Forest model in this thesis mainly for getting information 
about our features and how important they are for our model, and also, for 
making predictions (and comparing the performance with the Neural Network model). 
The feature importances helps us to see what features, i.e. what performance 
metrics are most relevant when performing an automatic evaluation for a student, 
so the staff of the course can work on improving the asessments that were not 
so relevant is the student's final examination grade.

Also, this can be used as a feature selection tool: the Random Forest model 
can provide a list of the most relevant features and then, another Machine 
Learning model can use this selected features as input. The outcome is that the 
model will give more accurate results and will have a better {\it running time}, 
mostly when we are dealing with a large datased and a complex learning model.

For the thesis, a Random Forest model with 10 estimators (random decision trees) 
was used for all the classification and regression approaches. The number of 
maximum features to select when buiding a node and finding the best split is 
$sqrt(N)$ for the classifiers, where $N$ is the size of the total feature 
space of the model. When using regression, the maximum features to consider 
is $N$. Different values were of this hyper-parameter were used and will be 
analyzed in the $5^{th}$ chapter of this report. 
